\chapter{Discussions}
\label{chap:4_discussions}

\section{Interpretation of the results}
The previous chapter described the conducted experiments in order to build the system taking decisions based on objective criterion. On this section, the obtained results will be interpreted, explaining their influence on the choices that have been made for obtaining the final proposal.\\

\subsection{Person detection results}

On \autoref{sec:3_test1}, the two outstanding object detection architectures were compared, using both to extract inferences on the same video sequence. The results can be visualized on \autoref{fig:3_test1_results}. The YOLO-based detector offers a slightly minor IoU than the SSD-based one (around 0.85 and 0.9 respectively), while taking 5 times less time to make inferences (35 ms vs. 175 ms). On these terms, the YOLO-based detector seems much more efficient. However, the IoU graph shows as well a very unstable detection in the YOLO case, being unable to detect the person in most cases. This is shows that this detector is too dependent on pose and lighting conditions for the detections to be successful. On the other hand, the SSD detector yields steady predictions, only cutting on the periods where the person was truly out of the field of view. Hence, this system is much more robust for our application scenario.\\

One fundamental requirement of the system is the real-time behavior, which makes inference time an important factor to be taken into account. However, as the system includes the described optical tracker, the YOLO detector can be discarded in favor of the SSD-based one, given that the YOLO version has a much lower detection rate\footnote{As it was described before, the deployed version of the YOLO detector is \textit{Tiny YOLOv3}, due to the memory requirements for deploying the full YOLOv3 model, which are higher than what the Jetson TX2 can handle. Thus, it is probable to expect a better performance on the full model in a different computer capable of handling it.} and this can not be palliated by the motion tracker.\\

\subsection{Face detection results}
Another important component in the neural pipeline is the face detection network. It has been tested in the experiment conducted in \autoref{sec:3_test2}, where it has been compared with the face detection tool used on the previous version of the system \cite{tfg}, a Haar cascade classifier \cite{violajones}. \autoref{fig:3_test2_results} shows the detection scores for the two mentioned systems on the same video sequence. It can be seen that both obtain similar IoU scores and drop at the same time when the person turns their back to the camera. However, the \texttt{faced} implementation (which uses deep learning to predict the face positions) is capable of keeping a non-zero IoU at several instants where the Haar performance drops to zero. This is due to pose variances of the person, as the main drawback of the Haar cascade classifier is that it is only capable of detecting frontal faces, dropping the performance whenever the person turns the face towards a side.\\

Hence, this test validates the improvement of the face detection performance when using a specific neural network trained for that purpose.

\subsection{Face recognition results}
The last component of the neural pipeline, the FaceNet face encoder, was tested as well on \autoref{sec:3_test4}, where the euclidean distances of the projections of two faces were compared to a reference face, corresponding to an external picture of one of them, the \textit{reference face} (\autoref{fig:3_test4_refface}). The results obtained on \autoref{fig:3_test4_result} allows to extract two conclusions about the quality of the projections of the faces:

\begin{itemize}
	\item The encodings of the reference person (the person with the same face than the reference one) have an overall remarkable stability. In average, the obtained projections for every frame are located at an approximate distance of $1.1$ (threshold chosen for accepting a person as the reference one). Exceptional rises in the distance can be found as well, but they are due to changes in the pose of the face, that reduce the quality of the projection.\\
	\item The encodings of a person different than the reference one have an overall higher distance from the reference face. This is convenient for avoiding false positives while determining that a face is the reference one.
\end{itemize} 

This allows to conclude a correct performance of the triplet loss (\autoref{fig:1_facenet_triplet_loss}) on which a FaceNet is trained \cite{facenet}. This yields an efficient separation between the encodings of different persons, as well as close encodings for faces belonging to the same person, making this system a robust approach to perform person recognition tasks.\\


\subsection{TensorRT results}

Once the neural pipeline is established and validated, the TensorRT optimizations can be tested. On \autoref{sec:3_test3}, \autoref{fig:3_std_vs_trt} illustrates the improvement between a standard graph and an optimized one. One of the premises of the optimization process is the reduction of the precision of the parameters in the neural network, which can be reduced from 64-bit values up to 16-bit or even 8-bit (performing an additional quantization process), as it was depicted on \autoref{sec:2_functional_architecture}.\\

The loss of precision is patent as well on \autoref{fig:3_std_vs_trt}, as the IoU between the standard graph and the optimized graph drops at several frames. However, this loss of precision is practically null, with some observable exceptions with a loss below 5\% of the original performance. However, the inference time gap can be observed as well. The difference is more notorious, as the TensorRT optimized model performs the inferences 3 times faster than the original graph.\\

Given these results, the TensorRT optimizations are a convenient tool to greatly increase the performance of the system, allowing the slower component (the neural pipeline) to experiment an important reduction on the inference time. As a result, the overall performance is greatly improved, receiving reliable neural updates more often.\\

As it was depicted on \autoref{sec:3_grid_trt}, a set of parameters can be tuned when optimizing a graph with TensorRT, yielding different performances. As \autoref{tab:3_ssd_trt_results} and \autoref{tab:3_yolo_trt_results} show, important reductions on the inference time can be obtained when the rest of parameters (\textit{Maximum Cached Engines} and \textit{Minimum Segment Size}) are tuned as well. However, these parameters only affect the inference time, as the precision loss is only due to the \textit{Precision Mode} parameter, which has been already analyzed.\\

The resulting models can be loaded in the program, instead of the original TensorFlow graphs, and offer an overall high performance.

\subsection{Motion tracker results}

Section \ref{sec:3_test5} describes the experiment designed for testing the improvement of the optical tracker used between inferences of the neural pipeline. This tracker aims to interpolate the person detections using optical flow, as well as solving the problem of partial occlusions.\\

The results on \autoref{fig:3_test5} show the IoU score between the persons and the ground truth labels on the test sequence. Regardless of the value of $k$ (the number of frames elapsed between neural detections), a similar performance can be appreciated under standard conditions (the faded portion of the graph). However, the emphasized region corresponds to an occlusion behind a hanging blanket (\autoref{fig:3_test5_frames}). On this lapse, a better performance is perceptible, especially when the inference time of the neural pipeline is higher ($k=20$). The hanging blanket occludes the person, causing the neural network to stop detecting her. However, as the tracker retains the detection for several updates because of the patience parameter, the person is not lost until several frames later. Additionally, the Lucas-Kanade algorithm allows to determine the displacement of the person even when it is not being detected by the neural pipeline. This explains the higher IoU when the tracker is active due to the bounding box shifting computed using Lucas-Kanade, confirming the improvement in the performance when using the tracker.\\

Additionally, while the neural pipeline runs on the GPU of the board, the Lucas-Kanade tracker, whose calculations are much lighter, runs on the CPU. This separation allows to combine both systems asynchronously without an affection on the overall load of the system.



\subsection{Global system results}

The full system can be seen in action on the recorded video sample available online\footnote{\url{https://www.youtube.com/watch?v=WZ0riKMwJWA}}. This video presents a sequence of the robot following the typical use case: the reference person enters into the field of view of the robot, showing their face to the camera. After some consecutive frames detecting the person, it is identified using the detected face. When its projection is close enough to the reference one, the robot starts following the person. For each frame, the linear and angular errors are computed, even if the face of the person is not seen anymore, as the system has checked previously that the person has to be followed. If the errors are outside the safe zones, a velocity command is computed and sent to the robot. This routine is executed iteratively until the person gets lost, causing the robot to stop waiting for the person to be seen again.



\section{Conclusions}

This section revisits the objectives stated in \autoref{sec:1_objectives}, and reviews the degree of accomplishment of each one of them\\

Regarding the first objective, this work has been focused on the development and testing of an embedded system that follows a reference person on a robust way, using the robustness of deep learning for being capable of working in real environments. This project has been developed using an accessible educational robot and a consumer RGBD sensor.\\

As the second objective requires, the detection and recognition pipeline has been exclusively designed using deep neural networks, ensuring a robust performance in non-controlled environments. As it has been seen along the project description, this robustness is crucial, especially because the image source is an image placed in a low height: the lens has an vertical inclination in order to see the full body of the persons in front of the robot. However, this causes as well an excessive amount of light from ceiling lamps to enter into the camera, dimming the persons on the image. As it has been tested in \autoref{chap:3_results}, classical systems tend to fail given this issue.\\


This neural pipeline has been complemented by a tracking component, improving the performance under unexpected issues, such as partial occlusions, or a higher inference time. This could happen if the networks are more complex or the inference device does not provide a low detection time. This fulfills the third objective of the project.\\

However, further improvements can be done on future works, such as:

\begin{itemize}
	\item Implement a multimodal tracking using sensor fusion, like in works such as \cite{rgbd_tracking}. The depth data of the person also provides information about their position, and bringing this information into the tracker can potentially lead to a better performance.
	
	\item Implement a probabilistic tracker, such as an EKF (\textit{Extended Kalman Filter}), taking leverage of the person trajectory. This can avoid confusions between two persons if they cross each other, or help the system to follow the trajectory of a person even if it is temporary lost. In addition, this can solve problems that come up from using optical flow, such as a person moving a part of their body. The displacement of the keypoints on that part of the body cause the full bounding box to suffer a displacement even if the person has not changed its position. This can be addressed using probabilistic subsystems to predict the movement of the person.
		
	\item Add a navigation component to the robot. The used robot is additionally equipped with a laser scanner, it can be used to detect possible obstacles between the robot and the person. Thus, a simple planning algorithm such as VFF (\textit{Virtual Force Field}) can be combined with this system in order to avoid collisions while the robot is moving.
\end{itemize}







